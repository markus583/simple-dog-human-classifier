{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "from PIL import Image, ImageFile\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download link for dataset: [dog dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip)\n",
    "\n",
    "Unzip and place it at /dogImages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8351 dog images.\n"
     ]
    }
   ],
   "source": [
    "# load filenames for human and dog images\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "dog_files = np.array(glob(\"dogImages/*/*/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('%d dog images.' % len(dog_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will try different approaches/models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-trained VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16 = models.vgg16(pretrained=True)\n",
    "# move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    VGG16 = VGG16.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(251, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def VGG16_predict(img_path):\n",
    "    ## Load and pre-process an image from the given img_path\n",
    "    ## Return index fo predicted class\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(img_path)\n",
    "    image = transform(image)  #.cuda()\n",
    "    pred = VGG16(image[None, ...])  # because batch is expected\n",
    "    return torch.argmax(pred) - 1 # predicted class index\n",
    "\n",
    "pred = VGG16_predict('dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg')\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    pred = VGG16_predict(img_path)\n",
    "    if 151 <= pred <= 268:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the face_detector algorithm \n",
    "# on the images in human_files_short and dog_files_short.\n",
    "human_count = 0\n",
    "for human_file in human_files_short:\n",
    "    out = dog_detector(human_file)\n",
    "    if out:\n",
    "        human_count += 1\n",
    "print(human_count)\n",
    "\n",
    "dog_count = 0\n",
    "for dog_file in dog_files_short:\n",
    "    out = dog_detector(dog_file)\n",
    "    if out:\n",
    "        dog_count += 1\n",
    "print(dog_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders for training, validation, and test sets\n",
    "train_files = np.array(glob(\"dogImages/train/*/*\"))\n",
    "test_files = np.array(glob(\"dogImages/test/*/*\"))\n",
    "val_files = np.array(glob(\"dogImages/valid/*/*\"))\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                      transforms.Resize(IMG_SIZE),\n",
    "                                      transforms.CenterCrop(IMG_SIZE),\n",
    "                                      transforms.RandomVerticalFlip(), # randomly flip and rotate\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                 std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(IMG_SIZE),\n",
    "                                     transforms.CenterCrop(IMG_SIZE),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                 std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "training = datasets.ImageFolder(\"dogImages/train\", transform=train_transform)\n",
    "validation = datasets.ImageFolder(\"dogImages/valid\", transform=test_transform)\n",
    "testing = datasets.ImageFolder(\"dogImages/test\",transform=test_transform)\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(testing,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(validation,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=50176, out_features=800, bias=True)\n",
       "  (fc2): Linear(in_features=800, out_features=133, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (batch_norm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the CNN architecture\n",
    "L1_IN = 3\n",
    "L1_OUT = 16\n",
    "L2_OUT = L1_OUT * 2\n",
    "L3_OUT = L2_OUT * 2\n",
    "FLATTEN = L3_OUT*28*28\n",
    "FC = 800\n",
    "BREEDS = len(training.classes)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "              # w*h is constant\n",
    "        self.conv1 = nn.Conv2d(L1_IN,L1_OUT,3,padding=1) #3,16\n",
    "        self.conv2 = nn.Conv2d(L1_OUT,L2_OUT,3,padding=1) #16,32\n",
    "        self.conv3 = nn.Conv2d(L2_OUT,L3_OUT,3,padding=1) #32,64\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(FLATTEN, FC)\n",
    "        self.fc2 = nn.Linear(FC, BREEDS)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(FC)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # flatten image\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # activation for first layer\n",
    "        x = self.dropout(F.relu(self.batch_norm(self.fc1(x))))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_scratch.cuda()\n",
    "model_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_scratch = optim.Adam(model_scratch.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PIL to be tolerant of image files that are truncated.\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    print(\"Started Training...\")\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # TRAINING #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "    \n",
    "        ######################    \n",
    "        # VALIDATION #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update avg. validation loss\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                # update avg validation loss \n",
    "                valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "                \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss \n",
    "    \n",
    "    # return trained model\n",
    "    print(\"Finished training\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = model_scratch\n",
    "save_path = \"model_scratch.pt\"\n",
    "\n",
    "# train the model\n",
    "model_scratch = train(epochs, loaders_scratch, model_scratch, optimizer_scratch, criterion_scratch, use_cuda, save_path)\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN using Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders same as above in 2.!\n",
    "loaders_transfer = loaders_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I chose ResNet50, but any other deep CNN may be chosen\n",
    "model_transfer = models.resnet50(pretrained=True)\n",
    "classifierInputs = model_transfer.fc.in_features\n",
    "\n",
    "print(classifierInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layers\n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# add output layer\n",
    "model_transfer.fc = nn.Linear(in_features=classifierInputs,\n",
    "                              out_features=BREEDS,\n",
    "                              bias=True)\n",
    "\n",
    "fc_parameters = model_transfer.fc.parameters()\n",
    "# unfreeze last (new) layer\n",
    "for param in fc_parameters:\n",
    "    param.requires_grad = True\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_transfer.to(device)\n",
    "\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optimizer.Adam(model_transfer.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "save_transfer = 'model_transfer.pt'\n",
    "# train model using function from above\n",
    "model_transfer = train(epochs, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, save_transfer)\n",
    "\n",
    "# load  model with best validation accuracy\n",
    "model_transfer.load_state_dict(torch.load(save_transfer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model using function from above\n",
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
